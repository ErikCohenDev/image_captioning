"""
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ct4uMp8ddv_4vDcZqFC7M6OFGrEN9x4F

<a href="https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_captioning.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# T81-558: Applications of Deep Neural Networks
**Module 10: Time Series in Keras**
* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)
* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/).

# Module 10 Material

* Part 10.1: Time Series Data Encoding for Deep Learning [[Video]](https://www.youtube.com/watch?v=dMUmHsktl04&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_1_timeseries.ipynb)
* Part 10.2: Programming LSTM with Keras and TensorFlow [[Video]](https://www.youtube.com/watch?v=wY0dyFgNCgY&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_2_lstm.ipynb)
* Part 10.3: Text Generation with Keras and TensorFlow [[Video]](https://www.youtube.com/watch?v=6ORnRAz3gnA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_3_text_generation.ipynb)
* **Part 10.4: Image Captioning with Keras and TensorFlow** [[Video]](https://www.youtube.com/watch?v=NmoW_AYWkb4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_4_captioning.ipynb)
* Part 10.5: Temporal CNN in Keras and TensorFlow [[Video]](https://www.youtube.com/watch?v=i390g8acZwk&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_10_5_temporal_cnn.ipynb)

# Google CoLab Instructions

The following code ensures that Google CoLab is running the correct version of TensorFlow.
  Running the following code will map your GDrive to ```/content/drive```.
"""

# Commented out IPython magic to ensure Python compatibility.
from tensorflow.keras.layers import (LSTM, Embedding,
                                     TimeDistributed, Dense, RepeatVector,
                                     Activation, Flatten, Reshape, concatenate,
                                     Dropout, BatchNormalization)
import tensorflow.keras.applications.mobilenet
from io import BytesIO
import requests
from matplotlib.pyplot import imshow
from PIL import Image, ImageFile
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import add
from tensorflow.keras.models import Model
from tensorflow.keras import optimizers
from tensorflow.keras import Input, layers
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.models import Sequential
from PIL import Image
import numpy as np
from time import time
import pickle
import tensorflow.keras.preprocessing.image
from tqdm import tqdm
import tensorflow.keras.applications.inception_v3
from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras.applications import MobileNet
import glob
import string
import os

"""# Part 10.4: Image Captioning with Keras and TensorFlow

Image captioning is a new technology that combines LSTM text generation with the computer vision powers of a convolutional neural network.  I first saw this technology in Andrej Karpathy's Dissertation. [[Cite:karpathy2016connecting]](https://cs.stanford.edu/people/karpathy/main.pdf) Figure 10.ACAP shows images from his work.

**Figure 10.ACAP: Andrej Karpathy's Dissertation**
![Captioning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/karpathy_thesis.jpg "Captioning")

In this part, we will use LSTM and CNN to create a basic image captioning system. We will use transfer learning to utilize these two projects:

* [InceptionV3](https://arxiv.org/abs/1512.00567)
* [Glove](https://nlp.stanford.edu/projects/glove/)

We use inception to extract features from the images.  Glove is a set of Natural Language Processing (NLP) vectors for common words. Figure 10.CAP-FLOW gives a high-level overview of captioning.

**Figure 10.CAP-FLOW: Captioning with a Neural Network**
![Captioning with a Neural Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-1.png "Captioning with a Neural Network")

We begin by importing the needed libraries.
"""


START = "startseq"
STOP = "endseq"
EPOCHS = 1
USE_INCEPTION = True

"""We use the following function to nicely format elapsed times."""

# Nicely formatted time string


def hms_string(sec_elapsed):
    h = int(sec_elapsed / (60 * 60))
    m = int((sec_elapsed % (60 * 60)) / 60)
    s = sec_elapsed % 60
    return f"{h}:{m:>02}:{s:>05.2f}"


"""### Needed Data

You will need to download the following data and place it in a folder for this example.  Point the *root_captioning* string at the folder that you are using for the caption generation. This folder should have the following sub-folders.

* data - Create this directory to hold saved models.
* [glove.6B](https://nlp.stanford.edu/projects/glove/) - Glove embeddings.
* [Flicker8k_Dataset](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip) - Flicker dataset.
* [Flicker8k_Text](https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip)

Note, the original Flickr datasets are no longer available, but you can download them from a location specified by [this article](https://machinelearningmastery.com/prepare-photo-caption-dataset-training-deep-learning-model/). 

### Running Locally

You will need to modify the following code to reflect the folder's location that you will store files required by this captions example.

"""

root_captioning = "./data/captions"

"""### Clean/Build Dataset From Flickr8k

We must pull in the Flickr dataset captions and clean them of extra whitespace, punctuation, and other distractions.
"""

null_punct = str.maketrans('', '', string.punctuation)
lookup = dict()

with open(os.path.join(root_captioning, 'Flickr8k_text',
                       'Flickr8k.token.txt'), 'r') as fp:

    max_length = 0
    for line in fp.read().split('\n'):
        tok = line.split()
        if len(line) >= 2:
            id = tok[0].split('.')[0]
            desc = tok[1:]

            # Cleanup description
            desc = [word.lower() for word in desc]
            desc = [w.translate(null_punct) for w in desc]
            desc = [word for word in desc if len(word) > 1]
            desc = [word for word in desc if word.isalpha()]
            max_length = max(max_length, len(desc))

            if id not in lookup:
                lookup[id] = list()
            lookup[id].append(' '.join(desc))

lex = set()
for key in lookup:
    [lex.update(d.split()) for d in lookup[key]]

"""The following code displays stats on the data downloaded and processed."""

print(len(lookup))  # How many unique words
print(len(lex))  # The dictionary
print(max_length)  # Maximum length of a caption (in words)

"""Next, we load the Glove embeddings."""

# Warning, running this too soon on GDrive can sometimes not work.
# Just rerun if len(img) = 0
img = glob.glob(os.path.join(root_captioning, 'Flicker8k_Dataset', '*.jpg'))

"""Display the count of how many Glove embeddings we have."""

len(img)

"""Read all image names and use the predefined train/test sets."""

train_images_path = os.path.join(root_captioning,
                                 'Flickr8k_text', 'Flickr_8k.trainImages.txt')
train_images = set(open(train_images_path, 'r').read().strip().split('\n'))
test_images_path = os.path.join(root_captioning,
                                'Flickr8k_text', 'Flickr_8k.testImages.txt')
test_images = set(open(test_images_path, 'r').read().strip().split('\n'))

train_img = []
test_img = []

for i in img:
    f = os.path.split(i)[-1]
    if f in train_images:
        train_img.append(f)
    elif f in test_images:
        test_img.append(f)

"""Display the size of the train and test sets."""

print(len(train_images))
print(len(test_images))

"""Build the sequences.  We include a **start** and **stop** token at the beginning/end.  We will later use the **start** token to begin the process of generating a caption.  Encountering the **stop** token in the generated text will let us know the process is complete."""

train_descriptions = {k: v for k, v in lookup.items() if f'{k}.jpg'
                      in train_images}
for n, v in train_descriptions.items():
    for d in range(len(v)):
        v[d] = f'{START} {v[d]} {STOP}'

"""See how many discriptions were extracted."""

len(train_descriptions)

"""### Choosing a Computer Vision Neural Network to Transfer

This example provides two neural networks that we can use via transfer learning.  In this example, I use Glove for the text embedding and InceptionV3 to extract features from the images.  Both of these transfers serve to extract features from the raw text and the images.  Without this prior knowledge transferred in, this example would take considerably more training.

I made it so you can interchange the neural network used for the images.  By setting the values **WIDTH**, **HEIGHT**, and **OUTPUT_DIM**, you can interchange images.  One characteristic that you are seeking for the image neural network is that it does not have too many outputs (once you strip the 1000-class imagenet classifier, as is common in transfer learning).  InceptionV3 has 2,048 features below the classifier, and MobileNet has over 50K.  If the additional dimensions truly capture aspects of the images, then they are worthwhile.  However, having 50K features increases the processing needed and the complexity of the neural network we are constructing.
"""

if USE_INCEPTION:
    encode_model = InceptionV3(weights='imagenet')
    encode_model = Model(encode_model.input, encode_model.layers[-2].output)
    WIDTH = 299
    HEIGHT = 299
    OUTPUT_DIM = 2048
    preprocess_input = \
        tensorflow.keras.applications.inception_v3.preprocess_input
else:
    encode_model = MobileNet(weights='imagenet', include_top=False)
    WIDTH = 224
    HEIGHT = 224
    OUTPUT_DIM = 50176
    preprocess_input = tensorflow.keras.applications.mobilenet.preprocess_input

"""The summary of the chosen image neural network to be transferred is displayed."""

encode_model.summary()

"""### Creating the Training Set

We need to encode the images to create the training set.  Later we will encode new images to present them for captioning.
"""


def encodeImage(img):
    # Resize all images to a standard size (specified bythe image
    # encoding network)
    img = img.resize((WIDTH, HEIGHT), Image.ANTIALIAS)
    # Convert a PIL image to a numpy array
    x = tensorflow.keras.preprocessing.image.img_to_array(img)
    # Expand to 2D array
    x = np.expand_dims(x, axis=0)
    # Perform any preprocessing needed by InceptionV3 or others
    x = preprocess_input(x)
    # Call InceptionV3 (or other) to extract the smaller feature set for
    # the image.
    x = encode_model.predict(x)  # Get the encoding vector for the image
    # Shape to correct form to be accepted by LSTM captioning network.
    x = np.reshape(x, OUTPUT_DIM)
    return x


"""We can how to generate the training set, which will involve looping over every JPG that we provided.  Because this can take a while to perform, we will save it to a pickle file.  This saved file prevents the considerable time needed to reprocess all of the images again.  Because the images are processed differently by different transferred neural networks, the filename contains the output dimensions. We follow this naming convention because if you changed from InceptionV3 to MobileNet, the number of output dimensions would change, and you must reprocess the images."""

train_path = os.path.join(root_captioning, "data", f'train{OUTPUT_DIM}.pkl')
if not os.path.exists(train_path):
    start = time()
    encoding_train = {}
    for id in tqdm(train_img):
        image_path = os.path.join(root_captioning, 'Flicker8k_Dataset', id)
        img = tensorflow.keras.preprocessing.image.load_img(image_path,
                                                            target_size=(HEIGHT, WIDTH))
        encoding_train[id] = encodeImage(img)
    with open(train_path, "wb") as fp:
        pickle.dump(encoding_train, fp)
    print(f"\nGenerating training set took: {hms_string(time()-start)}")
else:
    with open(train_path, "rb") as fp:
        encoding_train = pickle.load(fp)

"""We must also perform a similar process for the test images."""

test_path = os.path.join(root_captioning, "data", f'test{OUTPUT_DIM}.pkl')
if not os.path.exists(test_path):
    start = time()
    encoding_test = {}
    for id in tqdm(test_img):
        image_path = os.path.join(root_captioning, 'Flicker8k_Dataset', id)
        img = tensorflow.keras.preprocessing.image.load_img(image_path,
                                                            target_size=(HEIGHT, WIDTH))
        encoding_test[id] = encodeImage(img)
    with open(test_path, "wb") as fp:
        pickle.dump(encoding_test, fp)
    print(f"\nGenerating testing set took: {hms_string(time()-start)}")
else:
    with open(test_path, "rb") as fp:
        encoding_test = pickle.load(fp)

"""Next, we separate the captions that we will use for training.  There are two sides to this training, the images, and the captions."""

all_train_captions = []
for key, val in train_descriptions.items():
    for cap in val:
        all_train_captions.append(cap)
len(all_train_captions)

"""Words that do not occur very often can be misleading to neural network training.  It is better to remove such words.  Here we remove any words that occur less than ten times.  We display the new reduced size of the vocabulary shrunk."""

word_count_threshold = 10
word_counts = {}
nsents = 0
for sent in all_train_captions:
    nsents += 1
    for w in sent.split(' '):
        word_counts[w] = word_counts.get(w, 0) + 1

vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]
print('preprocessed words %d ==> %d' % (len(word_counts), len(vocab)))

"""Next, we build two lookup tables for this vocabulary. The table **idxtoword** converts index numbers to actual words to index values.  The **wordtoidx** lookup table performs the opposite."""

idxtoword = {}
wordtoidx = {}

ix = 1
for w in vocab:
    wordtoidx[w] = ix
    idxtoword[ix] = w
    ix += 1

vocab_size = len(idxtoword) + 1
vocab_size

"""Previously we added a start and stop token to all sentences.  We must account for this in the maximum length of captions."""

max_length += 2
print(max_length)

"""### Using a Data Generator

Up to this point, we've always generated training data ahead of time and fit the neural network to it.  It is not always practical to create all of the training data ahead of time.  The memory demands can be considerable.  If we generate the training data as the neural network needs it, it is possible to use a Keras generator.  The generator will create new data as it is needed.  The generator provided here creates the training data for the caption neural network, as it is needed.

If we were to build all needed training data ahead of time, it would look like Figure 10.CAP-WORK.

**Figure 10.CAP-WORK: Captioning Training Data**
![Captioning](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/caption-2.png "Captioning")

Here we are just training on two captions.  However, we would have to duplicate the image for each of these partial captions that we have.  Additionally, the Flikr8K data set has five captions for each picture.  Those would all require duplication of data as well.  It is much more efficient to generate the data as needed.
"""


def data_generator(descriptions, photos, wordtoidx,
                   max_length, num_photos_per_batch):
    # x1 - Training data for photos
    # x2 - The caption that goes with each photo
    # y - The predicted rest of the caption
    x1, x2, y = [], [], []
    n = 0
    while True:
        for key, desc_list in descriptions.items():
            n += 1
            photo = photos[key+'.jpg']
            # Each photo has 5 descriptions
            for desc in desc_list:
                # Convert each word into a list of sequences.
                seq = [wordtoidx[word] for word in desc.split(' ')
                       if word in wordtoidx]
                # Generate a training case for every possible sequence and outcome
                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    out_seq = to_categorical(
                        [out_seq], num_classes=vocab_size)[0]
                    x1.append(photo)
                    x2.append(in_seq)
                    y.append(out_seq)
            if n == num_photos_per_batch:
                yield ([np.array(x1), np.array(x2)], np.array(y))
                x1, x2, y = [], [], []
                n = 0


"""### Loading Glove Embeddings"""

glove_dir = os.path.join(root_captioning, 'glove.6B')
embeddings_index = {}
f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding="utf-8")

for line in tqdm(f):
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs

f.close()
print(f'Found {len(embeddings_index)} word vectors.')

"""### Building the Neural Network

We build an embedding matrix from Glove.  We will directly copy this matrix to the weight matrix of the neural network.
"""

embedding_dim = 200

# Get 200-dim dense vector for each of the 10000 words in out vocabulary
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in wordtoidx.items():
    # if i < max_words:
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # Words not found in the embedding index will be all zeros
        embedding_matrix[i] = embedding_vector

"""The matrix dimensions make sense.  It is 1652 (the size of the vocabulary) by 200 (the number of features Glove generates for each word)."""

embedding_matrix.shape

inputs1 = Input(shape=(OUTPUT_DIM,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)
caption_model = Model(inputs=[inputs1, inputs2], outputs=outputs)

embedding_dim

caption_model.summary()

caption_model.layers[2].set_weights([embedding_matrix])
caption_model.layers[2].trainable = False
caption_model.compile(loss='categorical_crossentropy', optimizer='adam')

"""### Train the Neural Network"""

number_pics_per_bath = 3
steps = len(train_descriptions)//number_pics_per_bath

model_path = os.path.join(root_captioning, "data", f'caption-model.hdf5')
if not os.path.exists(model_path):
    for i in tqdm(range(EPOCHS*2)):
        generator = data_generator(train_descriptions, encoding_train,
                                   wordtoidx, max_length, number_pics_per_bath)
        caption_model.fit_generator(generator, epochs=1,
                                    steps_per_epoch=steps, verbose=1)

    caption_model.optimizer.lr = 1e-4
    number_pics_per_bath = 6
    steps = len(train_descriptions)//number_pics_per_bath

    for i in range(EPOCHS):
        generator = data_generator(train_descriptions, encoding_train,
                                   wordtoidx, max_length, number_pics_per_bath)
        caption_model.fit_generator(generator, epochs=1,
                                    steps_per_epoch=steps, verbose=1)
    caption_model.save_weights(model_path)
    print(f"\Training took: {hms_string(time()-start)}")
else:
    caption_model.load_weights(model_path)

"""### Generating Captions

It is essential to understand that we do not generate a caption with one single call to the neural network's predict function.  Neural networks output a fixed-length tensor.  To get a variable-length output, such as free-form text, requires multiple calls to the neural network.

The neural network accepts two objects (which we map to the input neurons).  The first input is the photo, and the second input is an ever-growing caption.  The caption begins with just the starting token.  The neural network's output is the prediction of the next word in the caption.  The caption continues to grow until the neural network predicts an end token, or we reach the maximum length of a caption.  
"""


def generateCaption(photo):
    in_text = START
    for i in range(max_length):
        sequence = [wordtoidx[w] for w in in_text.split() if w in wordtoidx]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = caption_model.predict([photo, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = idxtoword[yhat]
        in_text += ' ' + word
        if word == STOP:
            break
    final = in_text.split()
    final = final[1:-1]
    final = ' '.join(final)
    return final


"""### Evaluate Performance on Test Data from Flicker8k

The caption model performs relatively well on images that are similar to the training set.
"""

for z in range(2):  # set higher to see more examples
    pic = list(encoding_test.keys())[z]
    image = encoding_test[pic].reshape((1, OUTPUT_DIM))
    print(os.path.join(root_captioning, 'Flicker8k_Dataset', pic))
    x = plt.imread(os.path.join(root_captioning, 'Flicker8k_Dataset', pic))
    plt.imshow(x)
    plt.show()
    print("Caption:", generateCaption(image))
    print("_____________________________________")

encoding_test[pic].shape

"""### Evaluate Performance on My Photos 

In the "photos" folder of this GitHub repository, I keep a collection of personal photos that I like to use for testing neural networks.  These are entirely separate from the Flicker8K dataset, and as a result, the caption neural network does not perform nearly as well.
"""

# Commented out IPython magic to ensure Python compatibility.

# %matplotlib inline

ROOT = "https://github.com/jeffheaton/" + \
    "t81_558_deep_learning/blob/master/photos/"
urls = [
    ROOT+"annie_dog.jpg?raw=true",
    ROOT+"landscape.jpg?raw=true",
    ROOT+"hickory_coat.jpg?raw=true"
]

for url in urls:
    response = requests.get(url)
    img = Image.open(BytesIO(response.content))
    img.load()

    plt.imshow(img)
    plt.show()

    response = requests.get(url)

    img = encodeImage(img).reshape((1, OUTPUT_DIM))
    print(img.shape)
    print("Caption:", generateCaption(img))
    print("_____________________________________")

"""# Module 10 Assignment

You can find the first assignment here: [assignment 10](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/assignments/assignment_yourname_class10.ipynb)
"""
